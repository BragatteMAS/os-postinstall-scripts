---
phase: 17-test-expansion-bash
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test-integration.bats
  - tests/contracts/api-parity.txt
  - tests/test-contracts.bats
autonomous: true

must_haves:
  truths:
    - "setup.sh --help shows usage information and exits"
    - "setup.sh --dry-run completes for each profile (minimal, developer, full) without making real changes"
    - "setup.sh with unknown flag shows error message containing the flag name"
    - "api-parity.txt maps all 5 paired Bash/PS modules with function-level correspondence"
    - "Contract validation confirms all Bash exported functions from paired modules appear in api-parity.txt"
  artifacts:
    - path: "tests/test-integration.bats"
      provides: "Integration tests for setup.sh CLI behavior"
      min_lines: 60
    - path: "tests/contracts/api-parity.txt"
      provides: "Declarative Bash/PowerShell API parity mapping"
      min_lines: 15
    - path: "tests/test-contracts.bats"
      provides: "Validation tests for contract file completeness"
      min_lines: 30
  key_links:
    - from: "tests/test-integration.bats"
      to: "setup.sh"
      via: "bash setup.sh with flags"
      pattern: 'bash.*setup\.sh'
    - from: "tests/test-contracts.bats"
      to: "tests/contracts/api-parity.txt"
      via: "grep validation against source exports"
      pattern: 'api-parity\.txt'
---

<objective>
Create integration tests for setup.sh CLI (~5-8 tests), the Bash/PS contract parity file, and contract validation tests.

Purpose: Integration tests verify that setup.sh --dry-run works end-to-end for all profiles, catching flag parsing, platform detection, and dispatch issues. The contract file documents cross-platform API correspondence for maintenance and future Pester test alignment.
Output: 1 integration test file, 1 contract file, 1 contract validation test file.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-test-expansion-bash/17-RESEARCH.md

@setup.sh
@src/core/logging.sh
@src/core/errors.sh
@src/core/platform.sh
@src/core/progress.sh
@src/platforms/windows/core/logging.psm1
@src/platforms/windows/core/errors.psm1
@src/platforms/windows/core/packages.psm1
@src/platforms/windows/core/idempotent.psm1
@src/platforms/windows/core/progress.psm1
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test-integration.bats (~6-8 tests)</name>
  <files>tests/test-integration.bats</files>
  <action>
Create `tests/test-integration.bats` for end-to-end CLI tests of setup.sh.

**setup() function:**
```bash
setup() {
    export NO_COLOR=1
    SETUP_SH="${BATS_TEST_DIRNAME}/../setup.sh"
}
```

No module sourcing needed -- integration tests invoke `bash setup.sh` as a subprocess.

**Tests to write:**

1. `setup.sh --help shows usage` - `run bash "$SETUP_SH" --help`, assert_success, assert_output --partial "Usage:", assert_output --partial "--dry-run"

2. `setup.sh --dry-run developer completes` - `run bash "$SETUP_SH" --dry-run developer`, assert_success, assert_output --partial "DRY RUN"

3. `setup.sh --dry-run minimal completes` - `run bash "$SETUP_SH" --dry-run minimal`, assert_success, assert_output --partial "DRY RUN"

4. `setup.sh --dry-run full completes` - `run bash "$SETUP_SH" --dry-run full`, assert_success, assert_output --partial "DRY RUN"

5. `setup.sh --dry-run shows detected platform` - `run bash "$SETUP_SH" --dry-run developer`, assert_output --partial "Detected:"

6. `setup.sh unknown flag shows error message` - `run bash "$SETUP_SH" --invalid-flag`, assert_output --partial "Unknown option", assert_output --partial "--invalid-flag". **DO NOT use assert_failure** -- the EXIT trap bug overrides exit code to 0 (see research Pitfall 5). Only check output content.

7. `setup.sh default profile is developer` - `run bash "$SETUP_SH" --dry-run`, assert_success, assert_output --partial "developer"

8. `setup.sh --dry-run shows completion summary` - `run bash "$SETUP_SH" --dry-run developer`, assert_output --partial "Complete" (matches "Dry Run Complete" or "Setup Complete")

**CRITICAL implementation notes:**
- ALL tests use `--dry-run` except test 1 (--help) and test 6 (unknown flag). NEVER run setup.sh without --dry-run in tests -- it would attempt real installations.
- Test 6 is the known EXIT trap bug case. Document with a comment: `# NOTE: Cannot assert_failure due to EXIT trap bug (see 17-RESEARCH.md Pitfall 5)`
- Tests 2-4 may take a few seconds each as they source the full module chain and run platform detection. This is expected.
- The `run bash` pattern creates a clean subprocess, so no source guard conflicts.
  </action>
  <verify>
Run: `./tests/lib/bats-core/bin/bats tests/test-integration.bats`
All 6-8 tests pass (0 failures).
  </verify>
  <done>test-integration.bats exists with 6+ passing tests covering --help, --dry-run for all 3 profiles, unknown flag error, default profile, and completion summary. All non-help tests use --dry-run.</done>
</task>

<task type="auto">
  <name>Task 2: Create api-parity.txt contract file and test-contracts.bats validation</name>
  <files>tests/contracts/api-parity.txt, tests/test-contracts.bats</files>
  <action>
**File 1: tests/contracts/api-parity.txt**

Create directory `tests/contracts/` and the contract file. Format: pipe-separated columns with header comments.

Content (use EXACTLY the mapping from research, which was verified against actual module exports):

```
# tests/contracts/api-parity.txt
# Bash/PowerShell API Parity Contract
# Format: bash_module | bash_function | ps_module | ps_function | behavior
#
# This file documents the mapping between Bash and PowerShell implementations.
# Bash has more granular functions; PS consolidates into fewer functions with parameters.
# This is by design (see docs/adr/ADR-006).
#
# Modules with PS equivalents: logging, errors, packages, idempotent, progress
# Modules without PS equivalents: platform (N/A), dotfiles (N/A), interactive (N/A)

logging.sh  | log_ok "msg"             | logging.psm1    | Write-Log -Level OK -Message "msg"       | outputs [OK] prefix
logging.sh  | log_error "msg"          | logging.psm1    | Write-Log -Level ERROR -Message "msg"    | outputs [ERROR] prefix to stderr
logging.sh  | log_warn "msg"           | logging.psm1    | Write-Log -Level WARN -Message "msg"     | outputs [WARN] prefix
logging.sh  | log_info "msg"           | logging.psm1    | Write-Log -Level INFO -Message "msg"     | outputs [INFO] prefix
logging.sh  | log_debug "msg"          | logging.psm1    | Write-Log -Level DEBUG -Message "msg"    | silent unless VERBOSE=true
logging.sh  | log_banner "n" "v"       | logging.psm1    | Write-Log -Level BANNER -Message "n"     | outputs === name === format
errors.sh   | record_failure "pkg"     | errors.psm1     | Add-FailedItem -Item "pkg"               | increments failure count
errors.sh   | show_failure_summary     | errors.psm1     | Show-FailureSummary                      | lists failed items or success msg
errors.sh   | get_failure_count        | errors.psm1     | Get-FailureCount                         | returns integer count
errors.sh   | clear_failures           | errors.psm1     | Clear-Failures                           | resets count to 0
errors.sh   | compute_exit_code        | errors.psm1     | Get-ExitCode                             | 0=success, 1=partial failure
packages.sh | load_packages "f.txt"    | packages.psm1   | Read-PackageFile -FileName "f.txt"       | reads package list, skips comments
idempotent.sh | is_installed "cmd"     | idempotent.psm1 | Test-WinGetInstalled -PackageId "id"     | checks if package installed
progress.sh | show_dry_run_banner      | progress.psm1   | Show-DryRunBanner                        | banner when DRY_RUN=true
progress.sh | count_platform_steps     | progress.psm1   | Get-PlatformStepCount                    | count platform-relevant files
progress.sh | show_completion_summary  | progress.psm1   | Show-CompletionSummary                   | end-of-run summary with duration
```

---

**File 2: tests/test-contracts.bats**

**setup() function:**
```bash
setup() {
    CONTRACT_FILE="${BATS_TEST_DIRNAME}/contracts/api-parity.txt"
    SRC_DIR="${BATS_TEST_DIRNAME}/../src"
}
```

No `load` of bats-support/bats-assert needed for basic tests, but include them for consistency:
```bash
load 'lib/bats-support/load'
load 'lib/bats-assert/load'
```

**Tests to write:**

1. `api-parity.txt exists and is non-empty` - `[ -f "$CONTRACT_FILE" ]`, `[ -s "$CONTRACT_FILE" ]`

2. `api-parity.txt is well-formed` - iterate non-comment non-empty lines, assert each has exactly 4 pipe characters (5 columns). On failure: `fail "Malformed line $line_num: $line"`

3. `all Bash exported functions from paired modules appear in contract` - for each paired module (logging.sh, errors.sh, packages.sh, idempotent.sh, progress.sh), extract the exported function names from `export -f` lines in the source file. For each function name, grep the contract file. Skip internal/alias functions (those starting with `_`, and logging aliases like `log`, `log_success`, `log_warning`, `info`, `error`, `warning`, `success`, `setup_colors`). The KEY exported functions that MUST appear:
   - logging.sh: log_ok, log_error, log_warn, log_info, log_debug, log_banner
   - errors.sh: record_failure, show_failure_summary, get_failure_count, clear_failures, compute_exit_code
   - packages.sh: load_packages
   - idempotent.sh: is_installed
   - progress.sh: show_dry_run_banner, count_platform_steps, show_completion_summary

   Implementation: define an array of "function_name|module" pairs, loop, grep contract file for each. This is more reliable than parsing `export -f` dynamically.

4. `all PS exported functions from paired modules appear in contract` - for each paired PS module, the expected functions are:
   - logging.psm1: Write-Log
   - errors.psm1: Add-FailedItem, Show-FailureSummary, Get-FailureCount, Clear-Failures, Get-ExitCode
   - packages.psm1: Read-PackageFile
   - idempotent.psm1: Test-WinGetInstalled
   - progress.psm1: Show-DryRunBanner, Get-PlatformStepCount, Show-CompletionSummary

   Implementation: same approach -- define expected PS functions, grep contract file.

**Note:** Only validate Bash side with source-file cross-reference. PS side is validated against the contract's own entries (not by running pwsh). This is a Bash test suite -- PS validation is informational per research recommendation.
  </action>
  <verify>
Run: `./tests/lib/bats-core/bin/bats tests/test-contracts.bats`
All 4 tests pass (0 failures).

Verify contract file exists: `[ -f tests/contracts/api-parity.txt ]`
Verify non-empty: `wc -l tests/contracts/api-parity.txt` (should show 16+ data lines + comments)
  </verify>
  <done>tests/contracts/api-parity.txt exists with 16 data lines mapping 5 paired Bash/PS modules. test-contracts.bats exists with 4 passing tests validating format, Bash export coverage, and PS export coverage. test-integration.bats exists with 6+ passing tests covering setup.sh CLI behavior.</done>
</task>

</tasks>

<verification>
Run ALL phase 17 tests together:
```bash
./tests/lib/bats-core/bin/bats tests/test-integration.bats tests/test-contracts.bats
```
Expected: ~10-12 tests, 0 failures.

Then the final full-suite verification (all .bats files):
```bash
./tests/lib/bats-core/bin/bats tests/*.bats
```
Expected: ~100+ total tests (44 existing + ~33-36 from Plan 01 + ~23-28 from Plan 02 + ~10-12 from Plan 03), 0 failures.

This satisfies Success Criterion 8 from the roadmap: "All tests pass: bats tests/*.bats exits 0"
</verification>

<success_criteria>
- test-integration.bats: 6+ tests passing (TEST-08)
- tests/contracts/api-parity.txt: exists with 16 data lines mapping 5 modules (TEST-09)
- test-contracts.bats: 4 tests passing validating contract format and completeness (TEST-09)
- All integration tests use --dry-run (no real system changes)
- Unknown flag test documents EXIT trap bug without relying on exit code
- All existing tests still pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/17-test-expansion-bash/17-03-SUMMARY.md`
</output>
